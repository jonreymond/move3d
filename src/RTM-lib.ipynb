{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 2D Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rtmlib in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (0.0.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rtmlib -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\liogr\\anaconda3\\envs\\move3d\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from rtmlib import Body, draw_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load C:\\Users\\liogr\\.cache\\rtmlib\\hub\\checkpoints\\yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.zip\" to C:\\Users\\liogr\\.cache\\rtmlib\\hub\\checkpoints\\rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.zip\n",
      "100%|██████████| 48.4M/48.4M [00:26<00:00, 1.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load C:\\Users\\liogr\\.cache\\rtmlib\\hub\\checkpoints\\rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.onnx with onnxruntime backend\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "#video_folder = os.path.join('..', '3dmotion', '20250326', 'camera')\n",
    "#output_folder = os.path.join('..', 'data', )  # Relative to where script is run\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "# Paths based on your setup\n",
    "video_folder = '/pvc/scratch/3dmotion/20250326/camera'\n",
    "output_folder = '/home/grienenb/move3d/data'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize pose estimation model\n",
    "body = Body(\n",
    "    mode='balanced',\n",
    "    backend='onnxruntime',\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load C:\\Users\\liogr\\.cache\\rtmlib\\hub\\checkpoints\\yolox_m_8xb8-300e_humanart-c2c7a14a.onnx with onnxruntime backend\n",
      "load C:\\Users\\liogr\\.cache\\rtmlib\\hub\\checkpoints\\rtmpose-m_simcc-body7_pt-body7_420e-256x192-e48f03d0_20230504.onnx with onnxruntime backend\n",
      "Processing: C0110.MP4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m keypoints, scores = \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m all_keypoints.append(keypoints.tolist())\n\u001b[32m     48\u001b[39m all_scores.append(scores.tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\liogr\\anaconda3\\envs\\move3d\\Lib\\site-packages\\rtmlib\\tools\\solution\\body.py:141\u001b[39m, in \u001b[36mBody.__call__\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    139\u001b[39m     keypoints, scores = \u001b[38;5;28mself\u001b[39m.pose_model(image)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     bboxes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     keypoints, scores = \u001b[38;5;28mself\u001b[39m.pose_model(image, bboxes=bboxes)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keypoints, scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\liogr\\anaconda3\\envs\\move3d\\Lib\\site-packages\\rtmlib\\tools\\object_detection\\yolox.py:29\u001b[39m, in \u001b[36mYOLOX.__call__\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray):\n\u001b[32m     28\u001b[39m     image, ratio = \u001b[38;5;28mself\u001b[39m.preprocess(image)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     30\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.postprocess(outputs, ratio)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\liogr\\anaconda3\\envs\\move3d\\Lib\\site-packages\\rtmlib\\tools\\base.py:127\u001b[39m, in \u001b[36mBaseTool.inference\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.session.get_outputs():\n\u001b[32m    125\u001b[39m         sess_output.append(out.name)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msess_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend == \u001b[33m'\u001b[39m\u001b[33mopenvino\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    129\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.compiled_model(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\liogr\\anaconda3\\envs\\move3d\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:270\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    268\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loop through videos\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith('.MP4'):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        output_video_path = os.path.join(output_folder, f'keypoints_{filename}')\n",
    "        output_json_path = os.path.join(output_folder, f'keypoints_{os.path.splitext(filename)[0]}.json')\n",
    "\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                      int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        frame_size = (width, height)\n",
    "\n",
    "        print(f\"  FPS: {fps}, Frame Size: {frame_size}\")\n",
    "        if fps == 0 or width == 0 or height == 0:\n",
    "            print(f\"Skipping {filename} due to invalid FPS or frame size.\")\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "        all_keypoints = []\n",
    "        all_scores = []\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            keypoints, scores = body(frame)\n",
    "            all_keypoints.append(keypoints.tolist())\n",
    "            all_scores.append(scores.tolist())\n",
    "\n",
    "            frame = draw_skeleton(frame, keypoints, scores, kpt_thr=0.5)\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count % 200 == 0:\n",
    "                print(f\"  Processed {frame_count} frames...\")\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Saved video: {output_video_path}\")\n",
    "\n",
    "        if frame_count == 0:\n",
    "            print(f\"No frames were written to {output_video_path}\") \n",
    "        else:\n",
    "            print(f\"Saved {frame_count} frames to {output_video_path}\")\n",
    "\n",
    "        # Save keypoints to JSON\n",
    "        json_data = {\n",
    "            \"video\": filename,\n",
    "            \"keypoints\": all_keypoints,\n",
    "            \"scores\": all_scores\n",
    "        }\n",
    "\n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "\n",
    "        print(f\"Saved keypoints JSON: {output_json_path}\")\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "move3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
